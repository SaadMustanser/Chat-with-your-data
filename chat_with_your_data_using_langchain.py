# -*- coding: utf-8 -*-
"""Chat with your Data using LangChain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-RT_kkFvYCHpgwSekDtjjoBgVXDghILb
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install langchain
!pip install --upgrade --quiet  langchain-google-genai pillow
!pip install pypdf
!pip install -U langchain-community
!pip install --upgrade --quiet  langchain sentence_transformers
!pip install -U langchain-community faiss-cpu langchain-openai tiktoken

import os
import sys
import getpass
import google.generativeai as genai
from google.colab import userdata
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory

if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Provide your Google API Key")

model = "gemini-pro"
llm = ChatGoogleGenerativeAI(model=model)

pdf_directory = "/content/drive/MyDrive/Resumes"

# List all PDF files in the directory
pdf_files = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory) if f.endswith('.pdf')]

# Initialize a list to store pages from all PDFs
all_pages = []

# Load each PDF file and extract pages
for pdf_file in pdf_files:
    loader = PyPDFLoader(pdf_file)
    pages = loader.load()
    for page_num, page in enumerate(pages, start=1):
        page.metadata['source'] = pdf_file
        page.metadata['page_number'] = page_num
    all_pages.extend(pages)

c_splitter = CharacterTextSplitter(
    separator='\n',
    chunk_size=1000,
    chunk_overlap=100
)
docs = c_splitter.split_documents(all_pages)

import getpass
inference_api_key = getpass.getpass("Enter your HF Inference API Key:\n\n")

def create_and_save_embeddings(documents=docs, embeddings_path="embeddings.faiss"):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = FAISS.from_documents(documents, embeddings)
    vector_store.save_local(embeddings_path)
    return vector_store

 # Ensure the directory exists
    os.makedirs(embeddings_path, exist_ok=True)

    # Save the FAISS index
    vector_store.save_local(embeddings_path)
    return vector_store

def load_embeddings(embeddings_path="embeddings.faiss"):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return FAISS.load_local(embeddings_path, embeddings, allow_dangerous_deserialization=True)

# Load or create the embeddings
try:
    vector_store = load_embeddings("embeddings.faiss")
except (FileNotFoundError, RuntimeError):  # Catch both file not found and runtime errors
    vector_store = create_and_save_embeddings(documents=docs, embeddings_path="embeddings.faiss")

# Set up the memory for conversation history
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Memory setup
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"  # Specify which key to store in memory
)

# Update setup_qa_chain to specify the correct output_key
def setup_qa_chain(vector_store, memory, temperature=1, k=11, threshold=50):
    prompt_template = PromptTemplate(
        template="Answer the following question based on the provided PDF content and our conversation so far. "
                 "If the information is not present, respond with 'I don't have this information. For more information, contact +123456789.'\n\n"
                 "Context: {context}\n\nQuestion: {question}\n\nAnswer:",
        input_variables=["context", "question"]
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        retriever=vector_store.as_retriever(search_kwargs={"k": k, "threshold": threshold}),
        llm=llm,
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt_template},
        output_key="answer"  # Specify the output key for the final answer
    )

    return qa_chain

llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=1)

# Now that vector_store and memory are defined, set up the QA chain
vector_store = create_and_save_embeddings(documents=docs, embeddings_path="embeddings.faiss")
qa_chain = setup_qa_chain(vector_store, memory, temperature=1, k=5, threshold=0.2)


def chatbot():
    print("Chatbot is ready! Type 'exit' to end the conversation.")
    while True:
        query = input("You: ")
        if query.lower() == "exit":
            print("Chatbot: Goodbye!")
            break
        result = qa_chain({"question": query})  # Use 'question' as input key
        print("Chatbot:", result['answer'])  # Use 'answer' as output key
        print("\nSources:")
        for doc in result['source_documents']:
            print(f"PDF: {doc.metadata['source']}, Page: {doc.metadata['page_number']}")

chatbot()

